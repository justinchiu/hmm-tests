# -*- coding: utf-8 -*-
"""kernelhmm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X5UDkWNjUn3azgvToDjQim_C2fZff0pX
"""

import math
import random

import torch
import torch_struct
import numpy as np

import banded_inference as inference

from utils import run_inference


#device = torch.device("cuda:0")
device = torch.device("cpu")
seed = 1234
torch.manual_seed(1234)
torch.cuda.manual_seed(1234)
np.random.seed(1234)
random.seed(1234)

N = 3 # batch size
T = 32 # length of sequence
V = 128 # vocab size

C = 256 # number of classes
H = 128 # embedding dimension
D = 64 # number of samples / projection dim

K = 8 # band size

start_emb = torch.randn(H, device=device)
state_emb = torch.randn(C, H, device=device)
next_state_emb = torch.randn(C, H, device=device)
preterminal_emb = torch.randn(C, H, device=device)
terminal_emb = torch.randn(V, H, device=device)
projection = torch.randn(H, D, device=device)
banded_transition = torch.randn(C, K+1, device=device)

start_emb.requires_grad = True
state_emb.requires_grad = True
next_state_emb.requires_grad = True
preterminal_emb.requires_grad = True
terminal_emb.requires_grad = True
projection.requires_grad = True
banded_transition.requires_grad = True


params = (
    start_emb, state_emb, next_state_emb, projection,
    preterminal_emb, terminal_emb,
    banded_transition,
)

# from_numpy api seems bad
text = torch.from_numpy(np.random.choice(V, size=(N, T))).to(device)
lengths = torch.from_numpy(np.random.choice(np.arange(T-3, T), size=(N,))).to(device)
lengths[0] = T
mask = torch.arange(T, device=device)[None] < lengths[:,None]
# no masking for now
print(text)
print(lengths)

print("BAND HMM")

print("torch struct inference")
evidence, grads = run_inference(text, params, inference.evidence_ts)
print(evidence)

print("fastbmm inference")
evidence, grads = run_inference(text, params, inference.evidence_fastbmm)
print(evidence)

